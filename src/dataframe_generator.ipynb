{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set infinite display\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the two metadata tables. Many case_ids are duplicated, but these are tied to the presence of multiple read_group_ids and so the duplicates are not filtered out at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert metadata tsvs into dataframes\n",
    "df = pd.read_csv('../../data/CPTAC-3.bio_repl_query.sur.tsv', sep='\\t', header=0)\n",
    "df2 = pd.read_csv('../../data/CPTAC-3.gdc_repl_query.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shape of the dataframe\n",
    "print(df.shape)\n",
    "# get shape of the dataframe\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicated case_id in df\n",
    "print(df.case_id.duplicated().sum())\n",
    "# check duplicated case_id in df2\n",
    "print(df2.case_id.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join both by case_id and keep all rows\n",
    "df3 = df.merge(df2, how='outer', on='case_id', suffixes=('_bio', '_gdc'))\n",
    "\n",
    "# get shape of the dataframe\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the column read_pair_number and submitted_unaligned_reads_id\n",
    "df3 = df3.drop(['read_pair_number'], axis=1)\n",
    "df3 = df3.drop(['submitted_unaligned_reads_id'], axis=1)\n",
    "\n",
    "# drop duplicated and save as new dataframe\n",
    "df4 = df3.drop_duplicates()\n",
    "\n",
    "# print shape of the old and new dataframes\n",
    "print(df3.shape)\n",
    "print(df4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate case_id\n",
    "df4[df4.duplicated(['case_id'], keep=False)].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few columns were found in both metadata tables and the following chunk of code makes sure values from both are kept in a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each pair of columns that were repeated in both initial dataframes and combine the values\n",
    "for col in df4.columns:\n",
    "    if col.endswith('_bio'):\n",
    "        gdc_col = col.replace('_bio', '_gdc')\n",
    "        if gdc_col in df4.columns:\n",
    "            new_col_name = col.replace('_bio', '')  # Create a new column name without suffixes\n",
    "            new_column_values = []\n",
    "            # Iterate down the rows in the DataFrame\n",
    "            for i in range(len(df4)):\n",
    "                bio_value = df4[col].iloc[i]\n",
    "                gdc_value = df4[gdc_col].iloc[i]\n",
    "                if not pd.isnull(bio_value):\n",
    "                    new_column_values.append(bio_value)\n",
    "                else:\n",
    "                    new_column_values.append(gdc_value)\n",
    "            df4[new_col_name] = new_column_values  # Add a new column with combined values\n",
    "            df4.drop([col, gdc_col], axis=1, inplace=True)  # Delete the original column pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following code starts from the unzipped contents of the .tgz file we got from box, unzips the subfiles if they haven't been unzipped already, and then compiles the contents and filenames into a TPM dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"../../data/local_results/\"  # Change this to the source directory where your .sf.gz files are located\n",
    "destination_dir = \"../../data/local_results/sf\"\n",
    "\n",
    "# Get a list of all files in the source directory\n",
    "filenames = os.listdir(source_dir)\n",
    "\n",
    "for filename in filenames:\n",
    "    if filename.endswith('.sf.gz'):\n",
    "        source_path = os.path.join(source_dir, filename)\n",
    "        destination_path = os.path.join(destination_dir, filename[:-3])\n",
    "\n",
    "        # Check if the destination file already exists\n",
    "        if not os.path.exists(destination_path):\n",
    "            with gzip.open(source_path, 'rb') as f_in:\n",
    "                with open(destination_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "        else:\n",
    "            print(f\"Skipping {filename} as it has already been unzipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the '.sf' files are located\n",
    "sf_directory = \"../../data/local_results/sf/\"\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# List the '.sf' files in the directory\n",
    "sf_files = [f for f in os.listdir(sf_directory) if f.endswith('.sf')]\n",
    "\n",
    "# loop through all .sf files\n",
    "for file in tqdm(sf_files):\n",
    "    # read in the file\n",
    "    df = pd.read_csv(\"../../data/local_results/sf/\" + file, sep='\\t', header=0)\n",
    "    # keep only the Name and TPM columns\n",
    "    df = df[['Name', 'TPM']]\n",
    "    # rename the TPM column to the filename without the .sf extension\n",
    "    df = df.rename(columns={'TPM': file.split('.')[0]})\n",
    "    # if results_df is empty, then set it to the df dataframe\n",
    "    if result_df.empty:\n",
    "        result_df = df\n",
    "    else:\n",
    "        # merge the dataframe with the result_df dataframe\n",
    "        result_df = pd.merge(result_df, df, on='Name', how='outer')\n",
    "    del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataframes are saved as .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results_df dataframe as a tsv file with the name TPM.tsv\n",
    "result_df.to_csv('../../results/TPM.tsv', sep='\\t', index=False)\n",
    "# save the metadata dataframe\n",
    "df4.to_csv('../../results/metadata.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
